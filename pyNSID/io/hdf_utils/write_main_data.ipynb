{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode = str\n",
    "class Dimension(object):\n",
    "    \"\"\"\n",
    "    ..autoclass::Dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, quantity, units, values, is_position):\n",
    "        \"\"\"\n",
    "        Simple object that describes a dimension in a dataset by its name, units, and values\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str or unicode\n",
    "            Name of the dimension. For example 'X'\n",
    "        quantity : str or unicode\n",
    "            Quantity for this dimension. For example: 'Length'\n",
    "        units : str or unicode\n",
    "            Units for this dimension. For example: 'um'\n",
    "        values : array-like or int\n",
    "            Values over which this dimension was varied. A linearly increasing set of values will be generated if an\n",
    "            integer is provided instead of an array.\n",
    "        is_position : bool\n",
    "            Whether or not this is a position or spectroscopy dimensions\n",
    "        \"\"\"\n",
    "        #name = validate_single_string_arg(name, 'name')\n",
    "        #quantity = validate_single_string_arg(quantity, 'quantity')\n",
    "\n",
    "        if not isinstance(units, (str, unicode)):\n",
    "            raise TypeError('units should be a string')\n",
    "        units = units.strip()\n",
    "\n",
    "        if isinstance(values, int):\n",
    "            if values < 1:\n",
    "                raise ValueError('values should at least be specified as a positive integer')\n",
    "            values = np.arange(values)\n",
    "        if not isinstance(values, (np.ndarray, list, tuple)):\n",
    "            raise TypeError('values should be array-like')\n",
    "        values = np.array(values)\n",
    "        if values.ndim > 1:\n",
    "            raise ValueError('Values for dimension: {} are not 1-dimensional'.format(name))\n",
    "\n",
    "        if not isinstance(is_position, bool):\n",
    "            raise TypeError('is_position should be a bool')\n",
    "\n",
    "        self.name = name\n",
    "        self.quantity = quantity\n",
    "        self.units = units\n",
    "        self.values = values\n",
    "        self.is_position = is_position\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{} - {} ({}): {}'.format(self.name, self.quantity, self.units, self.values)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Dimension):\n",
    "            if self.name != other.name:\n",
    "                return False\n",
    "            if self.units != other.units:\n",
    "                return False\n",
    "            if self.quantity != other.quantity:\n",
    "                return False\n",
    "            if len(self.values) != len(other.values):\n",
    "                return False\n",
    "            if not np.allclose(self.values, other.values):\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Y - Length (um): [ 0.   2.5  5.   7.5 10. ],\n",
       " 1: X - Length (um): [0. 1. 2. 3. 4. 5. 6.],\n",
       " 2: DC offset - Bias (V): [ 0.00000000e+00  5.87785252e-01  9.51056516e-01  9.51056516e-01\n",
       "   5.87785252e-01  1.22464680e-16 -5.87785252e-01 -9.51056516e-01\n",
       "  -9.51056516e-01 -5.87785252e-01 -2.44929360e-16],\n",
       " 3: BE Frequency - Frequency (Hz): [ 0.  5. 10.]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_dict = {0: Dimension('Y', 'Length', 'um', np.linspace(0, 10, num=5), True),\n",
    "        1: Dimension('X', 'Length', 'um', np.linspace(0, 6, num=7), True),\n",
    "        2: Dimension('DC offset', 'Bias', 'V', np.sin(np.linspace(0, 1, num=11) * 2 * np.pi), True),\n",
    "        3: Dimension('BE Frequency', 'Frequency', 'Hz', np.linspace(0, 10, num=3), True)}\n",
    "dim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = np.random.rand(5, 7, 11, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity = 'Blah'\n",
    "units = 'me'\n",
    "main_data_name = 'Raw_Data'\n",
    "kwargs = {'dtype': np.float}\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided numpy or Dask array for main_data OK so far\n",
      "(5, 7, 11, 3)\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(h5_parent_group, (h5py.Group, h5py.File)):\n",
    "    raise TypeError('h5_parent_group should be a h5py.File or h5py.Group object')\n",
    "if not is_editable_h5(h5_parent_group):\n",
    "    raise ValueError('The provided file is not editable')\n",
    "if verbose:\n",
    "    print('h5 group and file OK')\n",
    "\n",
    "quantity, units, main_data_name = validate_string_args([quantity, units, main_data_name],\n",
    "                                                       ['quantity', 'units', 'main_data_name'])\n",
    "\n",
    "if verbose:\n",
    "        print('quantity, units, main_data_name all OK')\n",
    "\n",
    "quantity = quantity.strip()\n",
    "units = units.strip()\n",
    "main_data_name = main_data_name.strip()\n",
    "if '-' in main_data_name:\n",
    "    warn('main_data_name should not contain the \"-\" character. Reformatted name from:{} to '\n",
    "         '{}'.format(main_data_name, main_data_name.replace('-', '_')))\n",
    "main_data_name = main_data_name.replace('-', '_')\n",
    "\n",
    "if isinstance(main_data, (list, tuple)):\n",
    "    if not contains_integers(main_data, min_val=1):\n",
    "        raise ValueError('main_data if specified as a shape should be a list / tuple of integers >= 1')\n",
    "    if len(main_data) < 1:\n",
    "        raise ValueError('main_data if specified as a shape should contain at least 1 number for the singular dimension')\n",
    "    if 'dtype' not in kwargs:\n",
    "        raise ValueError('dtype must be included as a kwarg when creating an empty dataset')\n",
    "    _ = validate_dtype(kwargs.get('dtype'))\n",
    "    main_shape = main_data\n",
    "    if verbose:\n",
    "        print('Selected empty dataset creation. OK so far')\n",
    "elif isinstance(main_data, (np.ndarray, da.core.Array)):\n",
    "    main_shape = main_data.shape\n",
    "    if verbose:\n",
    "        print('Provided numpy or Dask array for main_data OK so far')\n",
    "else:\n",
    "    raise TypeError('main_data should either be a numpy array or a tuple / list with the shape of the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate dimension list here\n",
    "# An N dimensional dataset should have N items in the dimension dictionary\n",
    "if len(dim_dict) != len(main_shape):\n",
    "    raise ValueError('Incorrect number of dimensions: {} provided to support main data, of shape: {}'\n",
    "                     ''.format(len(dim_list), main_shape))\n",
    "if set(range(len(main_shape))) != set(dim_dict.keys()):\n",
    "    raise KeyError('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item could either be a Dimension object or a HDF5 dataset\n",
    "# Collect the file within which these ancillary HDF5 objectsa are present if they are provided\n",
    "which_h5_file = []\n",
    "# Also collect the names of the dimensions. We want them to be unique\n",
    "dim_names = []\n",
    "for index, dim_exp_size in enumerate(main_shape):\n",
    "    this_dim = dim_dict[index]\n",
    "    if isinstance(this_dim, h5py.Dataset):\n",
    "        # All these checks should live in a helper function for cleaniness\n",
    "        # Is it 1D?\n",
    "        # Does this dataset have a \"simple\" dtype - no compound data type allowed!\n",
    "        # is the shape matching with the main dataset?\n",
    "        # Does it contain some ancillary attributes like 'name', quantity', 'units', and 'is_position' \n",
    "        # and are these of types str, str, str, and bool respectively and not empty?\n",
    "        # dim_names.append(this_name)\n",
    "        # are all datasets in the same file?\n",
    "        # which_h5_file.append(this_dim.file.???)\n",
    "        pass\n",
    "    elif isinstance(this_dim, Dimension):\n",
    "        # is the shape matching with the main dataset?\n",
    "        # Is there a HDF5 dataset with the same name already in the provided group where this dataset will be created?\n",
    "        # check if this object with the same name is a dataset and if it satisfies the above tests\n",
    "        # Otherwise, just append the dimension name for the uniqueness test\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError('Values of dim_dict should either be h5py.Dataset objects or Dimension. '\n",
    "                        'Object at index: {} was of type: {}'.format(index, this_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the names are all unqiue\n",
    "\n",
    "# Check to make sure that all ancillary datasets are in the same HDF5 file using which_h5_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dims = []\n",
    "# Now write Dimension objects to HDF5 datasets. \n",
    "for index in range(len(main_shape)):\n",
    "    this_dim = dim_dict[index]\n",
    "    if isinstance(this_dim, h5py.Dataset):\n",
    "        h5_dims.append(this_dim)\n",
    "    else: # We know by now that this is the Dimension object\n",
    "        # Write this dimension object to HDF5 dataset\n",
    "        h5_anc_dset = None\n",
    "        # Append this dataset to the list\n",
    "        h5_dims.append(h5_anc_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we have all the ancillary datasets\n",
    "# We write the main dataset now\n",
    "\n",
    "if h5_parent_group.file.driver == 'mpio':\n",
    "    if kwargs.pop('compression', None) is not None:\n",
    "        warn('This HDF5 file has been opened wth the \"mpio\" communicator. '\n",
    "             'mpi4py does not allow creation of compressed datasets. Compression kwarg has been removed')\n",
    "\n",
    "if isinstance(main_data, np.ndarray):\n",
    "    # Case 1 - simple small dataset\n",
    "    h5_main = h5_parent_group.create_dataset(main_data_name, data=main_data, **kwargs)\n",
    "    if verbose:\n",
    "        print('Created main dataset with provided data')\n",
    "elif isinstance(main_data, da.core.Array):\n",
    "    # Case 2 - Dask dataset\n",
    "    # step 0 - get rid of any automated dtype specification:\n",
    "    _ = kwargs.pop('dtype', None)\n",
    "    # step 1 - create the empty dataset:\n",
    "    h5_main = h5_parent_group.create_dataset(main_data_name, shape=main_data.shape, dtype=main_data.dtype,\n",
    "                                             **kwargs)\n",
    "    if verbose:\n",
    "        print('Created empty dataset: {} for writing Dask dataset: {}'.format(h5_main, main_data))\n",
    "        print('Dask array will be written to HDF5 dataset: \"{}\" in file: \"{}\"'.format(h5_main.name,\n",
    "                                                                                      h5_main.file.filename))\n",
    "    # Step 2 - now ask Dask to dump data to disk\n",
    "    da.to_hdf5(h5_main.file.filename, {h5_main.name: main_data})\n",
    "    # main_data.to_hdf5(h5_main.file.filename, h5_main.name)  # Does not work with python 2 for some reason\n",
    "else:\n",
    "    # Case 3 - large empty dataset\n",
    "    h5_main = h5_parent_group.create_dataset(main_data_name, main_data, **kwargs)\n",
    "    if verbose:\n",
    "        print('Created empty dataset for Main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_simple_attrs(h5_main, {'quantity': quantity, 'units': units})\n",
    "if verbose:\n",
    "    print('Wrote quantity and units attributes to main dataset')\n",
    "    \n",
    "if isinstance(main_dset_attrs, dict):\n",
    "    write_simple_attrs(h5_main, main_dset_attrs)\n",
    "    if verbose:\n",
    "        print('Wrote provided attributes to main dataset')\n",
    "\n",
    "write_book_keeping_attrs(h5_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and attach dimension scales for each ancillary dataset:\n",
    "\n",
    "if verbose:\n",
    "    print('Successfully linked datasets - dataset should be main now')\n",
    "\n",
    "for index, h5_dim in enumerate(h5_dims):\n",
    "    dim_name = get_attr(h5_dim, 'name')\n",
    "    # First make this HDF5 dataset a dimension scale\n",
    "    h5_dim.make_scale(dim_name)\n",
    "    # Attach the name of the dimension to the main dataset also\n",
    "    h5_main.dims[index].label = dim_name\n",
    "    # Finally attach the scale itself\n",
    "    h5_main.dims[index].attach_scale(h5_dim)\n",
    "    \n",
    "# Now the dataset should be a main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now return this object as a powerful object:    \n",
    "#from ..nsi_data import NSIDataset\n",
    "#return NSIDataset(h5_main)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
